
R version 3.5.2 (2018-12-20) -- "Eggshell Igloo"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> setwd("C:/Users/Mason Kulbaba/Dropbox/Rscripts/chamaecrista-adaptive-capacity/bootstrap/GC/2015")
> 
> # load packages
> library(aster)
Loading required package: trust
> 
> # load raster output
> load("rout2015.RData")
> 
> # load redata file
> load("redata2015.RData")
> 
> # load modmat.siredam matrix 
> load("modmat.siredam2015.RData")
> 
> modmat.siredamGC<- as.matrix(modmat.siredam2015)
> 
> #set random seed
> set.seed(51729)
> 
> 
> pred<- c(0,1,2,3,4) #make sure this matches your model
> 
> # designate family for each node in graphical model
> fam<- c(1,1,2,2,2) #make sure this matches your model
> 
> # generate "hat" estimates from data
> alpha.hat <- rout2015$alpha
> sigma.hat <- rout2015$sigma
> nu.hat <- rout2015$nu
> b.hat <- rout2015$b
> c.hat <- rout2015$c
> sout <- summary(rout2015)
> se.alpha.hat <- sout$alpha[ , "Std. Error"]
> se.sigma.hat <- sout$sigma[ , "Std. Error"]
> se.nu.hat <- sout$nu[ , "Std. Error"]
> 
> fixed <- rout2015$fixed
> random <- rout2015$random
> modmat.tot <- cbind(fixed, Reduce(cbind, random))
> 
> nfix <- ncol(fixed)
> nrand <- sapply(random, ncol)
> a.hat <- rep(sigma.hat, times = nrand)
> 
> # set outer (nboot) only
> nboot <- 500
> 
> ### prepare for boots function that performs a single bootstrap
> 
> alpha.star <- matrix(NaN, nboot, length(alpha.hat))
> sigma.star <- matrix(NaN, nboot, length(sigma.hat))
> nu.star <- matrix(NaN, nboot, length(nu.hat))
> se.alpha.star <- alpha.star
> se.sigma.star <- sigma.star
> se.nu.star <- nu.star
> 
> # make boots function
> 
> boots <- function(alpha, sigma) {
+   
+   a.hat <- rep(sigma, times = nrand)
+   
+   c.star <- rnorm(sum(nrand))
+   b.star <- a.hat * c.star
+   eff.star <- c(alpha, b.star)
+   phi.star <- as.numeric(as.vector(rout2015$obj$origin) +
+                            modmat.tot %*% eff.star)
+   theta.star <- astertransform(phi.star, rout2015$obj,
+                                to.cond = "conditional", to.mean = "canonical")
+   y.star <- raster(theta.star, pred, fam, rout2015$obj$root)
+   y.star <- as.vector(y.star)
+   
+   rout.star <- reaster(y.star ~ varb + fit : (block), # make sure this matches your model
+                        list(parental=~0 + fit:modmat.siredam2015),
+                        pred, fam, varb, id, root, data = redata2015,
+                        effects = c(alpha, c.star), sigma = sigma)
+ }
> 
> 
> # generate function to compute VaW estimate
> VaW<- function (rout2015){
+   bhat<- rout2015$b
+   bhat.sire<- bhat[grep("sireID", names(bhat))]
+   hoom.star <- predict(rout2015$obj,  newcoef = rout2015$alpha)
+   hoom.star<- matrix(hoom.star, ncol =5)
+   hoom.star<- hoom.star[ , 5]  
+   # mapping function
+   map <- function(b) {
+     stopifnot(length(b) == 1)
+     stopifnot(is.finite(b))
+     alpha <- rout2015$alpha
+     alpha[11] <- alpha[11] + b # adding fixed effect for fit:block6 (e.g. alpha[11]), make sure this matches your model
+     hoom.star <- predict(rout2015$obj, newcoef = alpha)
+     hoom.star<- matrix(hoom.star, ncol = 5)
+     return(hoom.star[1875, 5]) # return value of 5th node (seed #) for 2274th indiv (1st indiv in block 6). Change this to whatever your terminal fitness node is (e.g. fruit #)
+   }
+   fred<- Vectorize(map)
+   bhat.sire.mu<- fred(bhat.sire)  
+   hoom.star2<- predict(rout2015$obj, newcoef = rout2015$alpha, se.fit=TRUE, info.tol = 1e-13)
+   goom.star <- hoom.star2$gradient  
+   moom.star<- goom.star[,5]
+   moom.star<- matrix(moom.star, ncol=5)  
+   # calcualtion for Va(w) 
+   GC_Va<- 4*moom.star[1875 , 5]^2 * rout2015$nu[1]/map(0) # final calcuation of additive genetic variance for fitness
+   soutstar <- summary(rout2015)
+   GC_SE<- 4 * moom.star[1875, 5]^2 * soutstar$nu["parental", "Std. Error"] / map(0) 
+   vaandse <- c(GC_Va,GC_SE)
+   return(vaandse)
+ }
> 
> # SINGLE BOOTSTRAPPING
> 
> GC_Va.star <- rep(-1, nboot)
> se.GC_Va.star <- rep(-1, nboot)
> se.GC_Va.star_sd <- rep(-1, nboot)
> for (iboot in 1:nboot) {
+   
+   tryCatch({
+     rout.star <- boots(alpha.hat, sigma.hat)
+     if("try-error" %in% class(rout.star)) stop("reaster error (Nelder-Mead with pickle fail?)")
+   },error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
+   
+   
+   tryCatch({
+     vandse <- VaW(rout.star) # function for VaW calcualtion at bootstrap level 1
+     GC_Va.star[iboot] <- vandse[1] # function for VaW calcualtion at bootstrap level 1
+     se.GC_Va.star_sd[iboot]<- vandse[2] # calcualte standard deviation
+     if("try-error" %in% class(vandse)) stop("aster.predict error (direction of recession error?)") 
+   },error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
+   
+   
+   print(GC_Va.star[iboot]) # will print outer bootstrap estimate for each nboot interation
+   print(se.GC_Va.star_sd[iboot]) # will print outer bootstrap estimate for each nboot interation
+   print(iboot)
+   
+   # The current code generates a standard deviation scale metric
+   
+ }
[1] 1.700634
[1] 0.329924
[1] 1
[1] 1.886111
[1] 0.3659923
[1] 2
[1] 1.722833
[1] 0.3491404
[1] 3
[1] 2.530076
[1] 0.4909373
[1] 4
[1] 2.1368
[1] 0.4168551
[1] 5
[1] 2.566946
[1] 0.4822897
[1] 6
[1] 1.62906
[1] 0.323017
[1] 7
[1] 2.003705
[1] 0.3837183
[1] 8
[1] 2.175077
[1] 0.3971552
[1] 9
[1] 3.353496
[1] 0.5938227
[1] 10
[1] 2.746568
[1] 0.5331086
[1] 11
[1] 1.944118
[1] 0.3764778
[1] 12
[1] 2.892085
[1] 0.5396322
[1] 13
[1] 1.987362
[1] 0.3879497
[1] 14
[1] 1.838585
[1] 0.3813067
[1] 15
[1] 1.758631
[1] 0.3458652
[1] 16
[1] 4.403094
[1] 0.7688983
[1] 17
[1] 2.189202
[1] 0.4069223
[1] 18
[1] 3.595059
[1] 0.6791927
[1] 19
[1] 1.021311
[1] 0.2092385
[1] 20
[1] 2.724361
[1] 0.4983148
[1] 21
[1] 1.927613
[1] 0.368092
[1] 22
[1] 2.232141
[1] 0.4115217
[1] 23
[1] 1.17571
[1] 0.2481479
[1] 24
[1] 2.830504
[1] 0.5188397
[1] 25
[1] 4.013588
[1] 0.7269623
[1] 26
[1] 2.046756
[1] 0.4013968
[1] 27
[1] 2.920679
[1] 0.5424918
[1] 28
[1] 2.191419
[1] 0.4250157
[1] 29
[1] 2.745281
[1] 0.5122208
[1] 30
[1] 2.206664
[1] 0.4311043
[1] 31
[1] 2.764121
[1] 0.5355596
[1] 32
[1] 1.83348
[1] 0.3621469
[1] 33
[1] 2.853353
[1] 0.5442382
[1] 34
[1] 1.348947
[1] 0.2909497
[1] 35
[1] 1.240648
[1] 0.2402619
[1] 36
[1] 1.808712
[1] 0.3518096
[1] 37
[1] 2.755547
[1] 0.5077103
[1] 38
[1] 4.298421
[1] 0.7790612
[1] 39
[1] 2.405845
[1] 0.4445725
[1] 40
[1] 4.318924
[1] 0.8121328
[1] 41
[1] 3.032564
[1] 0.5356977
[1] 42
[1] 2.371572
[1] 0.4564869
[1] 43
[1] 1.025369
[1] 0.2087786
[1] 44
[1] 2.370548
[1] 0.4587287
[1] 45
[1] 2.605856
[1] 0.4882617
[1] 46
[1] 1.803
[1] 0.3507929
[1] 47
[1] 2.242691
[1] 0.4270092
[1] 48
[1] 1.809429
[1] 0.3622937
[1] 49
[1] 2.247246
[1] 0.4266137
[1] 50
[1] 1.520271
[1] 0.3131967
[1] 51
[1] 2.685216
[1] 0.5080865
[1] 52
[1] 1.749092
[1] 0.3437573
[1] 53
[1] 1.246253
[1] 0.2591709
[1] 54
[1] 2.00103
[1] 0.3879193
[1] 55
[1] 2.463675
[1] 0.4711118
[1] 56
[1] 1.181294
[1] 0.258718
[1] 57
[1] 1.817649
[1] 0.3689047
[1] 58
[1] 2.098113
[1] 0.408042
[1] 59
[1] 2.372903
[1] 0.464818
[1] 60
[1] 3.342965
[1] 0.6026521
[1] 61
[1] 1.626688
[1] 0.3393355
[1] 62
[1] 1.430928
[1] 0.2855941
[1] 63
[1] 2.208405
[1] 0.4056357
[1] 64
[1] 2.447452
[1] 0.4696223
[1] 65
[1] 2.201795
[1] 0.4324011
[1] 66
[1] 2.14554
[1] 0.4014447
[1] 67
[1] 3.162707
[1] 0.5971368
[1] 68
[1] 1.256423
[1] 0.2600881
[1] 69
[1] 2.831106
[1] 0.5233059
[1] 70
[1] 2.639393
[1] 0.4930878
[1] 71
[1] 1.396199
[1] 0.2748371
[1] 72
[1] 2.36069
[1] 0.4343781
[1] 73
[1] 2.558012
[1] 0.4603424
[1] 74
[1] 3.275353
[1] 0.5984664
[1] 75
[1] 2.268745
[1] 0.4278183
[1] 76
[1] 1.466118
[1] 0.2912927
[1] 77
[1] 2.301669
[1] 0.4545502
[1] 78
[1] 2.020463
[1] 0.3807108
[1] 79
[1] 2.18762
[1] 0.4367508
[1] 80
[1] 2.098157
[1] 0.3984079
[1] 81
[1] 2.151767
[1] 0.4173275
[1] 82
[1] 2.4641
[1] 0.4432718
[1] 83
[1] 2.331798
[1] 0.4509799
[1] 84
[1] 2.451535
[1] 0.4761683
[1] 85
[1] 2.07104
[1] 0.390274
[1] 86
[1] 1.992163
[1] 0.3715436
[1] 87
[1] 1.823163
[1] 0.3706316
[1] 88
[1] 2.447493
[1] 0.4872278
[1] 89
[1] 2.249123
[1] 0.4315489
[1] 90
[1] 2.15402
[1] 0.4297187
[1] 91
[1] 3.668617
[1] 0.6642758
[1] 92
[1] 1.456241
[1] 0.2881736
[1] 93
[1] 2.314199
[1] 0.4621538
[1] 94
[1] 1.819853
[1] 0.3434785
[1] 95
[1] 1.661414
[1] 0.323733
[1] 96
[1] 2.217188
[1] 0.4149419
[1] 97
[1] 1.273491
[1] 0.2709917
[1] 98
[1] 2.474524
[1] 0.4731901
[1] 99
[1] 2.875362
[1] 0.5374179
[1] 100
[1] 2.990233
[1] 0.5423111
[1] 101
[1] 1.578802
[1] 0.3022909
[1] 102
[1] 2.455639
[1] 0.4875959
[1] 103
[1] 3.037465
[1] 0.5463559
[1] 104
[1] 1.948218
[1] 0.3679304
[1] 105
[1] 3.330926
[1] 0.6127219
[1] 106
[1] 1.679725
[1] 0.3328072
[1] 107
[1] 3.963626
[1] 0.6858631
[1] 108
[1] 2.342056
[1] 0.443938
[1] 109
[1] 2.431597
[1] 0.473082
[1] 110
[1] 2.799914
[1] 0.5235279
[1] 111
[1] 1.264554
[1] 0.2802821
[1] 112
[1] 2.261464
[1] 0.4232385
[1] 113
[1] 2.259324
[1] 0.4219039
[1] 114
[1] 2.189355
[1] 0.4204741
[1] 115
[1] 1.657917
[1] 0.3552774
[1] 116
[1] 2.156538
[1] 0.4033434
[1] 117
[1] 3.032427
[1] 0.5765011
[1] 118
[1] 2.23387
[1] 0.4150315
[1] 119
[1] 2.521
[1] 0.4695484
[1] 120
[1] 1.552296
[1] 0.331455
[1] 121
[1] 2.062512
[1] 0.4022887
[1] 122
[1] 1.937953
[1] 0.3838998
[1] 123
[1] 1.827068
[1] 0.3868425
[1] 124
[1] 1.451985
[1] 0.2923408
[1] 125
[1] 2.162682
[1] 0.4046585
[1] 126
[1] 0.7275639
[1] 0.1621182
[1] 127
[1] 1.713137
[1] 0.3338048
[1] 128
[1] 1.636162
[1] 0.3146179
[1] 129
[1] 1.695129
[1] 0.3464337
[1] 130
[1] 3.944272
[1] 0.7169434
[1] 131
[1] 1.603336
[1] 0.3077325
[1] 132
[1] 2.79037
[1] 0.5237807
[1] 133
[1] 1.605695
[1] 0.3244836
[1] 134
[1] 2.555832
[1] 0.4561984
[1] 135
[1] 3.043584
[1] 0.5463352
[1] 136
[1] 1.562499
[1] 0.3197573
[1] 137
[1] 1.460751
[1] 0.3041004
[1] 138
[1] 2.149936
[1] 0.3986868
[1] 139
[1] 2.928595
[1] 0.5081833
[1] 140
[1] 2.752828
[1] 0.5160647
[1] 141
[1] 1.908415
[1] 0.3893967
[1] 142
[1] 2.096619
[1] 0.4006433
[1] 143
[1] 1.864001
[1] 0.3576204
[1] 144
[1] 2.944299
[1] 0.5501039
[1] 145
[1] 3.7869
[1] 0.6987323
[1] 146
[1] 2.710032
[1] 0.4965916
[1] 147
[1] 2.02274
[1] 0.3996944
[1] 148
[1] 3.033176
[1] 0.5557994
[1] 149
[1] 1.921137
[1] 0.4033124
[1] 150
[1] 1.303575
[1] 0.2710765
[1] 151
[1] 1.922732
[1] 0.3733277
[1] 152
[1] 2.40814
[1] 0.475691
[1] 153
[1] 3.372428
[1] 0.5886689
[1] 154
[1] 3.078864
[1] 0.5600257
[1] 155
[1] 3.15617
[1] 0.5796083
[1] 156
[1] 2.642943
[1] 0.4962448
[1] 157
[1] 1.755216
[1] 0.3424105
[1] 158
[1] 3.200776
[1] 0.6011477
[1] 159
[1] 1.626606
[1] 0.3114295
[1] 160
[1] 4.285881
[1] 0.7695169
[1] 161
[1] 1.857741
[1] 0.3552606
[1] 162
[1] 2.678487
[1] 0.5158131
[1] 163
[1] 1.397321
[1] 0.2788796
[1] 164
[1] 2.214657
[1] 0.4254495
[1] 165
[1] 2.306895
[1] 0.45146
[1] 166
[1] 2.34604
[1] 0.4612638
[1] 167
[1] 2.101058
[1] 0.4066632
[1] 168
[1] 1.518101
[1] 0.3052881
[1] 169
[1] 1.748528
[1] 0.3450042
[1] 170
[1] 1.098288
[1] 0.2346614
[1] 171
[1] 2.414515
[1] 0.4526881
[1] 172
[1] 2.482364
[1] 0.4746539
[1] 173
[1] 3.030887
[1] 0.5491161
[1] 174
[1] 1.507163
[1] 0.3098517
[1] 175
[1] 1.635188
[1] 0.3228849
[1] 176
[1] 1.537402
[1] 0.3245338
[1] 177
[1] 3.192166
[1] 0.5713942
[1] 178
[1] 2.951214
[1] 0.5602114
[1] 179
[1] 3.420118
[1] 0.6119116
[1] 180
[1] 1.536862
[1] 0.308436
[1] 181
[1] 1.90456
[1] 0.3821551
[1] 182
[1] 2.395892
[1] 0.4603771
[1] 183
[1] 2.300271
[1] 0.4265869
[1] 184
[1] 1.735305
[1] 0.3518068
[1] 185
[1] 3.370124
[1] 0.6141883
[1] 186
[1] 1.659404
[1] 0.3450261
[1] 187
[1] 1.91364
[1] 0.3758527
[1] 188
[1] 3.604174
[1] 0.6716615
[1] 189
[1] 1.972628
[1] 0.3588748
[1] 190
[1] 2.259028
[1] 0.460925
[1] 191
[1] 1.141267
[1] 0.2367419
[1] 192
[1] 2.116609
[1] 0.4448052
[1] 193
[1] 1.546449
[1] 0.326568
[1] 194
[1] 2.584685
[1] 0.4715109
[1] 195
[1] 2.353789
[1] 0.4585073
[1] 196
[1] 2.944217
[1] 0.5428469
[1] 197
[1] 1.709101
[1] 0.3304422
[1] 198
[1] 2.383565
[1] 0.4515086
[1] 199
[1] 2.07757
[1] 0.4063318
[1] 200
[1] 2.782876
[1] 0.5567508
[1] 201
[1] 2.460985
[1] 0.465611
[1] 202
[1] 1.856743
[1] 0.4023589
[1] 203
[1] 1.658335
[1] 0.3264984
[1] 204
[1] 2.038762
[1] 0.4036279
[1] 205
[1] 2.427117
[1] 0.4749541
[1] 206
[1] 2.2953
[1] 0.426974
[1] 207
[1] 3.377624
[1] 0.6202851
[1] 208
[1] 1.994513
[1] 0.3610436
[1] 209
[1] 3.587316
[1] 0.6694621
[1] 210
[1] 3.221534
[1] 0.6010924
[1] 211
[1] 1.932407
[1] 0.371594
[1] 212
[1] 2.13855
[1] 0.4095824
[1] 213
[1] 2.296045
[1] 0.442197
[1] 214
[1] 1.646214
[1] 0.3094327
[1] 215
[1] 2.23533
[1] 0.436211
[1] 216
[1] 2.195736
[1] 0.411466
[1] 217
[1] 2.190046
[1] 0.4071139
[1] 218
[1] 2.100474
[1] 0.405061
[1] 219
[1] 2.394351
[1] 0.4431758
[1] 220
[1] 2.325247
[1] 0.4307045
[1] 221
[1] 2.74653
[1] 0.5078848
[1] 222
[1] 3.153857
[1] 0.6030154
[1] 223
[1] 1.764084
[1] 0.3471807
[1] 224
[1] 2.375062
[1] 0.4643915
[1] 225
[1] 1.362534
[1] 0.2682848
[1] 226
[1] 3.026112
[1] 0.5699632
[1] 227
[1] 1.969514
[1] 0.3942584
[1] 228
[1] 1.308508
[1] 0.2745722
[1] 229
[1] 1.391646
[1] 0.2941818
[1] 230
[1] 1.853405
[1] 0.3718134
[1] 231
[1] 1.543451
[1] 0.3260792
[1] 232
[1] 3.702075
[1] 0.6686897
[1] 233
[1] 1.176183
[1] 0.2443804
[1] 234
[1] 2.190855
[1] 0.4134444
[1] 235
[1] 3.591174
[1] 0.6575915
[1] 236
[1] 2.075009
[1] 0.3992297
[1] 237
[1] 2.032607
[1] 0.3918174
[1] 238
[1] 1.385591
[1] 0.2748518
[1] 239
[1] 1.666297
[1] 0.3378905
[1] 240
[1] 4.161656
[1] 0.7226841
[1] 241
[1] 1.514569
[1] 0.3089892
[1] 242
[1] 2.566299
[1] 0.4608353
[1] 243
[1] 1.816979
[1] 0.3737178
[1] 244
[1] 0.4902169
[1] 0.1076985
[1] 245
[1] 2.068463
[1] 0.3951515
[1] 246
[1] 2.124521
[1] 0.4144284
[1] 247
[1] 1.079643
[1] 0.2251521
[1] 248
[1] 3.612556
[1] 0.655286
[1] 249
[1] 1.422495
[1] 0.3064396
[1] 250
[1] 1.484138
[1] 0.3292275
[1] 251
[1] 2.303785
[1] 0.4261799
[1] 252
[1] 3.08627
[1] 0.5927086
[1] 253
[1] 2.252732
[1] 0.4311585
[1] 254
[1] 2.293044
[1] 0.4296613
[1] 255
[1] 3.366188
[1] 0.6386502
[1] 256
[1] 1.997189
[1] 0.4026778
[1] 257
[1] 2.747044
[1] 0.5058285
[1] 258
[1] 2.357597
[1] 0.4412464
[1] 259
[1] 1.421172
[1] 0.2955382
[1] 260
[1] 2.191274
[1] 0.4478735
[1] 261
[1] 1.809339
[1] 0.3638734
[1] 262
[1] 2.68146
[1] 0.475807
[1] 263
[1] 2.562852
[1] 0.4933813
[1] 264
[1] 1.694092
[1] 0.3369023
[1] 265
[1] 1.642077
[1] 0.3303496
[1] 266
[1] 4.452178
[1] 0.8044852
[1] 267
[1] 1.280878
[1] 0.2604972
[1] 268
[1] 2.662032
[1] 0.5345193
[1] 269
[1] 2.454908
[1] 0.457482
[1] 270
[1] 1.066143
[1] 0.2343877
[1] 271
[1] 2.511004
[1] 0.4808268
[1] 272
[1] 1.090535
[1] 0.226179
[1] 273
[1] 2.25743
[1] 0.4359331
[1] 274
[1] 2.432686
[1] 0.4858084
[1] 275
[1] 2.306929
[1] 0.4356402
[1] 276
[1] 2.066294
[1] 0.3870072
[1] 277
[1] 1.971067
[1] 0.3759351
[1] 278
[1] 1.147498
[1] 0.2496863
[1] 279
[1] 4.570403
[1] 0.8547325
[1] 280
[1] 2.096114
[1] 0.3802317
[1] 281
[1] 3.313943
[1] 0.5846988
[1] 282
[1] 2.761098
[1] 0.5176878
[1] 283
[1] 2.883699
[1] 0.5248095
[1] 284
[1] 3.136765
[1] 0.5567861
[1] 285
[1] 2.779843
[1] 0.5301243
[1] 286
[1] 2.427262
[1] 0.4534702
[1] 287
[1] 0.9386428
[1] 0.1948238
[1] 288
[1] 1.503621
[1] 0.2919684
[1] 289
[1] 3.238141
[1] 0.5676506
[1] 290
[1] 3.643676
[1] 0.6400635
[1] 291
[1] 4.475475
[1] 0.8083697
[1] 292
[1] 1.621877
[1] 0.3387624
[1] 293
[1] 3.350678
[1] 0.6242515
[1] 294
[1] 1.179994
[1] 0.2918452
[1] 295
[1] 1.042294
[1] 0.2222888
[1] 296
[1] 3.259507
[1] 0.5848543
[1] 297
[1] 2.955013
[1] 0.5521018
[1] 298
[1] 1.827107
[1] 0.3447035
[1] 299
[1] 1.810827
[1] 0.3471788
[1] 300
[1] 3.657581
[1] 0.6688757
[1] 301
[1] 4.862377
[1] 0.8751151
[1] 302
[1] 3.575847
[1] 0.6187493
[1] 303
[1] 1.806748
[1] 0.3571188
[1] 304
[1] 2.156141
[1] 0.4104626
[1] 305
[1] 2.473349
[1] 0.4855045
[1] 306
[1] 2.997011
[1] 0.549204
[1] 307
[1] 1.511247
[1] 0.2912666
[1] 308
[1] 2.456658
[1] 0.4696338
[1] 309
[1] 1.001215
[1] 0.2039635
[1] 310
[1] 2.442421
[1] 0.4612845
[1] 311
[1] 2.207791
[1] 0.4297978
[1] 312
[1] 2.147389
[1] 0.4264415
[1] 313
[1] 3.345096
[1] 0.6149454
[1] 314
[1] 3.346553
[1] 0.6168724
[1] 315
[1] 1.657235
[1] 0.324617
[1] 316
[1] 2.194759
[1] 0.4108769
[1] 317
[1] 1.711286
[1] 0.327115
[1] 318
[1] 1.884298
[1] 0.3554271
[1] 319
[1] 2.159332
[1] 0.4107962
[1] 320
[1] 2.012652
[1] 0.3804244
[1] 321
[1] 2.468149
[1] 0.449221
[1] 322
[1] 2.839453
[1] 0.5282353
[1] 323
[1] 3.100357
[1] 0.5689996
[1] 324
[1] 2.558582
[1] 0.4865631
[1] 325
[1] 2.062422
[1] 0.4115279
[1] 326
[1] 2.005993
[1] 0.3908027
[1] 327
[1] 2.086672
[1] 0.4042731
[1] 328
[1] 3.214539
[1] 0.5831667
[1] 329
[1] 2.161096
[1] 0.4090981
[1] 330
[1] 3.250077
[1] 0.5920972
[1] 331
[1] 1.400421
[1] 0.2853432
[1] 332
[1] 2.186287
[1] 0.4137342
[1] 333
[1] 3.540104
[1] 0.6486445
[1] 334
[1] 2.685828
[1] 0.4899561
[1] 335
[1] 1.895592
[1] 0.3727965
[1] 336
[1] 3.457383
[1] 0.5976631
[1] 337
[1] 2.176308
[1] 0.4197815
[1] 338
[1] 2.001496
[1] 0.3999851
[1] 339
[1] 1.094761
[1] 0.2261569
[1] 340
[1] 2.067707
[1] 0.3996059
[1] 341
[1] 4.231511
[1] 0.7678803
[1] 342
[1] 2.310119
[1] 0.438781
[1] 343
[1] 2.099639
[1] 0.4072606
[1] 344
[1] 2.977638
[1] 0.5373609
[1] 345
[1] 1.751199
[1] 0.3665328
[1] 346
[1] 2.723815
[1] 0.5036405
[1] 347
[1] 1.75606
[1] 0.3579703
[1] 348
[1] 2.477868
[1] 0.4620787
[1] 349
[1] 1.576162
[1] 0.3216408
[1] 350
[1] 3.236011
[1] 0.6044265
[1] 351
[1] 2.526611
[1] 0.473788
[1] 352
[1] 2.928794
[1] 0.5288791
[1] 353
[1] 3.827859
[1] 0.6917916
[1] 354
[1] 2.035692
[1] 0.3857066
[1] 355
[1] 3.111157
[1] 0.5707802
[1] 356
[1] 1.853861
[1] 0.3666049
[1] 357
[1] 2.68615
[1] 0.4929756
[1] 358
[1] 2.385876
[1] 0.4475694
[1] 359
[1] 2.670795
[1] 0.5017601
[1] 360
[1] 2.390857
[1] 0.4769972
[1] 361
[1] 2.083127
[1] 0.388653
[1] 362
[1] 2.065352
[1] 0.3865636
[1] 363
[1] 1.436142
[1] 0.2838443
[1] 364
[1] 2.707397
[1] 0.5144222
[1] 365
[1] 5.165212
[1] 0.8928606
[1] 366
[1] 1.830892
[1] 0.3368566
[1] 367
[1] 2.439263
[1] 0.466501
[1] 368
[1] 1.395415
[1] 0.2917663
[1] 369
[1] 3.157859
[1] 0.5790215
[1] 370
[1] 1.553161
[1] 0.3198867
[1] 371
[1] 2.112383
[1] 0.4050543
[1] 372
[1] 2.683985
[1] 0.4983318
[1] 373
[1] 2.731703
[1] 0.5354547
[1] 374
[1] 1.715
[1] 0.3599071
[1] 375
[1] 2.380614
[1] 0.4454543
[1] 376
[1] 3.194904
[1] 0.5953719
[1] 377
[1] 1.931698
[1] 0.3561944
[1] 378
[1] 1.37991
[1] 0.3009527
[1] 379
[1] 2.683738
[1] 0.5118925
[1] 380
[1] 1.82549
[1] 0.3476763
[1] 381
[1] 1.579198
[1] 0.2976572
[1] 382
[1] 2.306024
[1] 0.4336648
[1] 383
[1] 2.007949
[1] 0.3864824
[1] 384
[1] 2.770077
[1] 0.5130802
[1] 385
[1] 3.867829
[1] 0.6978112
[1] 386
[1] 3.34644
[1] 0.6034956
[1] 387
[1] 2.11714
[1] 0.4408049
[1] 388
[1] 1.619586
[1] 0.3063969
[1] 389
[1] 2.25122
[1] 0.4466907
[1] 390
[1] 2.830034
[1] 0.5149407
[1] 391
[1] 3.05174
[1] 0.5524936
[1] 392
[1] 3.340378
[1] 0.609526
[1] 393
[1] 1.346423
[1] 0.2633637
[1] 394
[1] 1.302167
[1] 0.2992296
[1] 395
[1] 1.33755
[1] 0.2978636
[1] 396
[1] 2.619883
[1] 0.4988458
[1] 397
[1] 2.102785
[1] 0.4101649
[1] 398
[1] 3.210201
[1] 0.5773198
[1] 399
[1] 3.625989
[1] 0.6602061
[1] 400
[1] 2.04305
[1] 0.407456
[1] 401
[1] 1.144957
[1] 0.2315893
[1] 402
[1] 1.837017
[1] 0.3651623
[1] 403
[1] 3.608177
[1] 0.675594
[1] 404
[1] 2.443133
[1] 0.459736
[1] 405
[1] 2.865004
[1] 0.5313931
[1] 406
[1] 2.150422
[1] 0.4378525
[1] 407
[1] 1.118335
[1] 0.238858
[1] 408
[1] 1.967378
[1] 0.3826792
[1] 409
[1] 1.457694
[1] 0.2802604
[1] 410
[1] 2.006205
[1] 0.3961773
[1] 411
[1] 2.288369
[1] 0.4238306
[1] 412
[1] 1.719307
[1] 0.332476
[1] 413
[1] 2.814566
[1] 0.5250035
[1] 414
[1] 2.446136
[1] 0.4616683
[1] 415
[1] 2.090054
[1] 0.3969708
[1] 416
[1] 3.767143
[1] 0.6984969
[1] 417
[1] 2.145329
[1] 0.4111325
[1] 418
[1] 2.659915
[1] 0.521018
[1] 419
[1] 1.99943
[1] 0.3923584
[1] 420
[1] 2.558974
[1] 0.4735409
[1] 421
[1] 2.066031
[1] 0.4087584
[1] 422
[1] 1.467995
[1] 0.289649
[1] 423
[1] 3.165476
[1] 0.5948839
[1] 424
[1] 2.549684
[1] 0.5087805
[1] 425
[1] 2.192925
[1] 0.4282254
[1] 426
[1] 2.539031
[1] 0.479707
[1] 427
[1] 1.813149
[1] 0.3624185
[1] 428
[1] 2.481557
[1] 0.4515772
[1] 429
[1] 4.487339
[1] 0.7821499
[1] 430
[1] 2.702664
[1] 0.4851355
[1] 431
[1] 2.48741
[1] 0.4685907
[1] 432
[1] 1.30496
[1] 0.2716709
[1] 433
[1] 2.411806
[1] 0.4606522
[1] 434
[1] 1.808068
[1] 0.3620324
[1] 435
[1] 1.535854
[1] 0.2945201
[1] 436
[1] 2.50607
[1] 0.4594395
[1] 437
[1] 1.376331
[1] 0.277061
[1] 438
[1] 3.182224
[1] 0.594509
[1] 439
[1] 3.202158
[1] 0.6020569
[1] 440
[1] 2.176172
[1] 0.403209
[1] 441
[1] 2.316097
[1] 0.4474553
[1] 442
[1] 4.027629
[1] 0.716055
[1] 443
[1] 2.358525
[1] 0.4363493
[1] 444
[1] 1.230338
[1] 0.2539843
[1] 445
[1] 1.573892
[1] 0.3127476
[1] 446
[1] 3.174321
[1] 0.589345
[1] 447
[1] 3.556594
[1] 0.6463311
[1] 448
[1] 2.301351
[1] 0.4317069
[1] 449
[1] 3.265465
[1] 0.6332062
[1] 450
[1] 1.962809
[1] 0.3773748
[1] 451
[1] 3.791791
[1] 0.6932122
[1] 452
[1] 2.678177
[1] 0.4812405
[1] 453
[1] 2.673837
[1] 0.49965
[1] 454
[1] 0.9474955
[1] 0.2103385
[1] 455
[1] 2.260971
[1] 0.4120476
[1] 456
[1] 3.099175
[1] 0.5606073
[1] 457
[1] 2.317209
[1] 0.4153959
[1] 458
[1] 2.138328
[1] 0.4040391
[1] 459
[1] 1.646235
[1] 0.3173988
[1] 460
[1] 2.139887
[1] 0.3995619
[1] 461
[1] 1.309531
[1] 0.2688486
[1] 462
[1] 1.925778
[1] 0.3716369
[1] 463
[1] 1.755009
[1] 0.3640278
[1] 464
[1] 1.619826
[1] 0.3227417
[1] 465
[1] 1.529478
[1] 0.3173381
[1] 466
[1] 1.582438
[1] 0.3226114
[1] 467
[1] 1.603671
[1] 0.3159445
[1] 468
[1] 1.995036
[1] 0.3670436
[1] 469
[1] 2.322853
[1] 0.4349514
[1] 470
[1] 2.419547
[1] 0.4666988
[1] 471
[1] 1.721136
[1] 0.3321456
[1] 472
[1] 2.007965
[1] 0.3838043
[1] 473
[1] 2.851216
[1] 0.554938
[1] 474
[1] 0.9353005
[1] 0.1945038
[1] 475
[1] 0.9659982
[1] 0.2162812
[1] 476
[1] 2.063068
[1] 0.393208
[1] 477
[1] 2.203896
[1] 0.4208138
[1] 478
[1] 3.274322
[1] 0.6280761
[1] 479
[1] 2.473426
[1] 0.4845399
[1] 480
[1] 1.978013
[1] 0.3764631
[1] 481
[1] 1.874811
[1] 0.3666578
[1] 482
[1] 2.090952
[1] 0.3970597
[1] 483
[1] 2.805827
[1] 0.5211276
[1] 484
[1] 2.952633
[1] 0.5386774
[1] 485
[1] 3.089843
[1] 0.5534603
[1] 486
[1] 1.688476
[1] 0.3463605
[1] 487
[1] 3.645698
[1] 0.6541167
[1] 488
[1] 3.110463
[1] 0.5514413
[1] 489
[1] 1.419009
[1] 0.297328
[1] 490
[1] 2.852892
[1] 0.5178408
[1] 491
[1] 1.626242
[1] 0.3174294
[1] 492
[1] 1.362083
[1] 0.2611939
[1] 493
[1] 3.856332
[1] 0.7150321
[1] 494
[1] 1.685561
[1] 0.3276121
[1] 495
[1] 2.19435
[1] 0.4076571
[1] 496
[1] 2.638175
[1] 0.492941
[1] 497
[1] 2.038691
[1] 0.3972611
[1] 498
[1] 2.800308
[1] 0.5060979
[1] 499
[1] 2.753467
[1] 0.4959345
[1] 500
> 
> # GENERATE BOOTSTRAP t CONFIDENCE INTERVALS
> 
> 
> # Save Rdata for entire bootstrap run
> save.image("singleboot_GC2015_run_500_2.RData")
> 
> # Generate Va(W) from data
> GC_Va.hat <- VaW(rout2015)[1] # generates va from data
> GC_Va.hat_sd <- VaW(rout2015)[2] # using the empirical sd.
> 
> # set confidence level
> conf.level <- 0.95
> 
> # calculate t for standard deviation-based confidence interval
> t <- (GC_Va.star - GC_Va.hat) / se.GC_Va.star_sd
> 
> # generate critical values with t for standard deviation-based confidence interval
> crit <- quantile(t, probs = c((1 - conf.level) / 2, (1 + conf.level) / 2))
> 
> # Conf. Int. for GC_Va with standard deviation scale metric
> GC_Va.hat - rev(crit) * GC_Va.hat_sd
    97.5%      2.5% 
0.5446233 3.3197884 
> 
> 
> #Or with raw variance
> quantile(GC_Va.star,c(0.025,0.975))
    2.5%    97.5% 
1.092543 4.020960 
> 
> 
> 
> proc.time()
     user    system   elapsed 
585528.59   2842.68 591820.53 
