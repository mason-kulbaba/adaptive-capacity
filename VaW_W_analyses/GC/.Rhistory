redata <- reshape(gcf, varying = list(vars), direction = "long",timevar = "varb", times = as.factor(vars), v.names = "resp")
#Designation of fitness variable: "totalseeds"
fit <- grepl("totalseeds", as.character(redata$varb))
fit<- as.numeric(fit)
redata$fit <- fit
#set root in redata
redata<- data.frame(redata, root=1)
library(aster)
#set graphical model and distributions
#Germ->FLW->total.pods->pods.collected->Seed.ct
pred<- c(0,1,2,3,4)
fam<- c(1,1,2,2,2)
#run fixed effect aster model
aoutf<- aster(resp~varb + fit:(block), pred, fam, varb, id, root, data=redata)
summary(aoutf, show.graph=TRUE)
pout<- predict.aster(aoutf, se.fit=TRUE)
#make up covariate data for hypothetical
#indivs that meet "typical" criteria
##############
#for fun, attempting to make an estimate for each block
id<- unique(redata$id)
fred <- data.frame(block=levels(as.factor(redata$block)),
Germ=1, flw=1, total.pods =1,total.pods.collected=1,totalseeds=1,root = 1)
renewdata <- reshape(fred, varying = list(vars),
direction = "long", timevar = "varb",
times = as.factor(vars), v.names = "resp")
layer<- gsub("[0-9]", "", as.character(renewdata$varb))
renewdata<- data.frame(renewdata, layer= layer)
fit<- as.numeric(layer=="totalseeds")
renewdata<- droplevels(renewdata)
renewdata<- data.frame(renewdata, fit = fit)
#can make CI for these "typical" estimaes of fitness
pout<- predict(aoutf, newdata= renewdata, varvar= varb, gradient=TRUE, #should "gradient=TRUE" be here?
amat=amat,  idvar = id, root = root, se.fit = TRUE)
#put the parameter estimates into a matrix with individuals in rows
#and nodes along columns
nnode<- length(vars)
sally<- matrix(pout$fit, ncol = nnode)
dim(sally)#yes! 8 x 5 matrix: 8 indiv by 5 nodes
#name the rows (by block id) and columns (as nodes)
rownames(sally)<- unique(as.character(renewdata$block))
colnames(sally)<- unique(as.character(renewdata$varb))
#using just seed set ("totalseeds"), the file "herman"
#has the desired estimates of expected fitness for a "typical" individual
#in each block.  NOTE: still need to generate SE for these estimates
herman<- sally[,grepl("totalseeds", colnames(sally))]
herman
#try to get mean fitness estiamtes and standard errors for predictions with scaling up
#by accounting for subsampling at planting stage
#load col. vector of individual seed sampling proportions
p<- read.csv("C:/Users/Mason Kulbaba/Dropbox/Rscripts/Final_VaW_Wbar/data/GC.2015.P.csv")
np<- as.integer(length(p))# length of col. vector  for individual
nindv<- as.integer(nrow(gcf)) #this many individuals in our data: 3665
nblock<- nrow(fred) #this is from previous version and not used below
nnode<- length(vars) #length of graphical model
amat<- array(0, c(nindv, nnode, np))
dim(amat) # amat is a 3665 x 5 x 1 array
foo<- grepl("totalseeds", vars)
for(k in 1:np)
amat[k, foo, k]<- 1
## Error Here: "amat is array but dimensions 1 and 2 do not match modmat"
pout.amat<- predict(aoutf, newdata= renewdata, varvar= varb,
idvar= id, gradient=TRUE, root = root, se.fit=TRUE, amat = amat)
#combine estimates withs td.err  , and then round
#to three decimal places
foo<- cbind(pout.amat$fit, pout.amat$se.fit)
rownames(foo)<- as.character(fred$nindv)
colnames(foo)<- c("W", "std.err.")
GC_block_fit_scaled<- round(foo, 3)
GC_block_fit_scaled
dim(amat) # amat is a 3665 x 5 x 1 array
## Error Here: "amat is array but dimensions 1 and 2 do not match modmat"
pout.amat<- predict(aoutf, newdata= renewdata, varvar= varb,
idvar= id, gradient=TRUE, root = root, se.fit=TRUE, amat = amat)
#can make CI for these "typical" estimaes of fitness
pout<- predict(aoutf, newdata= renewdata, varvar= varb, gradient=TRUE, #should "gradient=TRUE" be here?
idvar = id, root = root, se.fit = TRUE)
dim(pout$modmat
dim(pout$modmat)
pout$modmat
amat<- array(0, c(nindv, nnode, p))
head(p)
amat<- array(0, c(nindv, nnode, p))
p2<- as.numeric(p)
p2<- as.vector(p)
class(p2)
amat<- array(0, c(nindv, nnode, p2))
library(rmarkdown)
library(ggplot2)
format(Sys.time(), '%B %d, %Y')
?unclass
data(cars)
names(cars)
x<- rnorm(100)
y<- rnorm(100)
head(x)
head(y)
y<- abs(rnorm(100))
x<- abs(rnorm(100))
pos<- cbind(x, y)
pos
head(pos)
pos
head(pos)
x<- abs(rnorm(100))
x2<- abs(rnorm(100))
x1<- abs(rnorm(100))
x2<- abs(rnorm(100))
y1<- abs(rnorm(100))
y2<- abs(rnorm(100))
pos<- cbind(x1, x2, y1,y2)
head(pos, n=25)
pos$dist<- sqrt((x1-x2)^2 + (y1 - y2)^2)
head(pos)
dist<- sqrt((x1-x2)^2 + (y1 - y2)^2)
head(dist)
dim(dist)
length(list)
length(dist)
pos2<- cbind(pos, dist)
dim(pos)
head(pos)
x1<- abs(rnorm(100))
x2<- abs(rnorm(100))
y1<- abs(rnorm(100))
y2<- abs(rnorm(100))
pos<- cbind(x1, x2, y1,y2)
dist<- sqrt((x1-x2)^2 + (y1 - y2)^2)
pos2<- cbind(pos, dist)
head(pos2)
den<- density(pos2$dist)
class(pos2)
pos2<- as.data.frame(pos2)
den<- density(pos2$dist)
den
plot(den)
hist(pos2$dist)
fin<- read.csv("C:/Users/Mason Kulbaba/Dropbox/Rscripts/density/data/aster.dat.csv")
#change class of factor variables
fin$Den<- as.factor(fin$Den)
fin$Gen<- as.factor(fin$Gen)
fin$plotID<- as.factor(fin$plotID)
fin$plantID<- as.factor(fin$plantID)
fin$familyID<- as.factor(fin$familyID)
#fin$aborted<- as.numeric(fin$aborted)
lm1<- lm(aborted ~ Gen, data=fin)
lm2<- lm(aborted ~ Gen + Den, data=fin)
lm3<- lm(aborted ~ Gen + Den + Den*Gen, data=fin)#interaction not significant, but is sig. with aster (see above section)
summary(lm3)
pairs(emmeans(lm3, "Den", "Gen", type='response'))
library(emmeans)
install.packages("emmeans"
)
library(emmeans)
pairs(emmeans(lm3, "Den", "Gen", type='response'))
plot(emmeans(lm3, "Den", "Gen", type='response'))
pairs(emmeans(lm3, "Den", "Gen", type='response'))
emmeans(lm3, "Den", "Gen", type='response')
emmeans(lm2, "Den", "Gen", type='response')
pairs(emmeans(lm2, "Den", "Gen", type='response'))
emmeans(lm2, "Den", "Gen")
?emmeans
lm2<- glm(aborted ~ Gen + Den, data=fin)
summary(lm2)
emmeans(lm2, "Den", "Gen")
emmeans(lm2, "Den", "Gen", type="response")
lm2<- lm(aborted ~ Gen + Den, data=fin)
emmeans(lm2, "Den", "Gen", type="response")
pairs(emmeans(lm2, "Den", "Gen", type='response'))
?lme
emmeans(lm2, type="response")
emmeans(lm2, specs = c("Gen", "Den"), type="response")
lsmeans(lm2, specs = c("Gen", "Den"), type="response")
pairs(emmeans(lm2, "Den", "Gen", type='response'))
lm2<- lm(log(aborted) ~ Gen + Den, data=fin)
class(fin$aborted)
table(is.na(fin$aborted))
fin$aborted<- as.numeric(fin$aborted)
lm2<- lm(log(aborted) ~ Gen + Den, data=fin)
lm2<- glm(log(aborted) ~ Gen + Den, data=fin)
lm2<- glm(log10(aborted) ~ Gen + Den, data=fin)
predict.aster
library(aster)
?predict.aster
for (i in 1:10){
tryCatch({
print[i]
if (i ==7) error("found seven!")
})
}
for (i in 1:10){
tryCatch({
print(i)
if (i ==7) error("found seven!")
})
}
for (i in 1:10){
tryCatch({
print(i)
if (i ==7) error("found seven!")
else next
})
}
for (i in 1:10){
tryCatch({
print(i)
if (i ==7) stop("found seven!")
else next
})
}
for (i in 1:10){
tryCatch({
print(i)
if (i ==7) stop("found seven!")
})
}
for (i in 1:10){
tryCatch({
print(i)
if (i ==7) stop("found seven!")
}, function(e){})
}
for (i in 1:10){
for (i in 1:10){
for (i in 1:10){
num<- nrand(100)
setwd("C:/Users/Mason Kulbaba/Dropbox/Rscripts/chamaecrista-adaptive-capacity/VaW_W_analyses/GC")
#Load reaster
#2015 reaster data
load(file="rout2015.RData")
#2016 reaster data
load(file="rout2016.RData")
#2017 reaster data
load(file="rout2017.RData")
library(aster)
#########################
#Estimate Va(W) for 2015#
#########################
#extract bhat - the estimates of random effects
bhat<- rout2015$b
#extract the sire effects
bhat.sire<- bhat[grep("paternalID", names(bhat))]
#stem plot of bhat estimates
stem(bhat.sire)#the canonical estimates "look" somewhat normal...as they should
hoom <- predict(rout2015$obj, newcoef = rout2015$alpha)
hoom<- matrix(hoom, ncol =5)
hoom<- hoom[ , 5]
#add a component of b_hat that we want to map to the mean value
#parameter scale (from canoonical scale) to fit:block1 fixed effect (alpha[11]= block6),
#predict, and take value of fourth node for first indiv.
map <- function(b) {
stopifnot(length(b) == 1)
stopifnot(is.finite(b))
alpha <- rout2015$alpha
alpha[11] <- alpha[11] + b #block 6 effects
hoom <- predict(rout2015$obj, newcoef = alpha)
hoom <- matrix(hoom, ncol = 5)
return(hoom[1874, 5])#first individual in block 6, fifth node
}
#vectorize mapping function
fred<- Vectorize(map)
#plot the curve
curve(fred, from = -1 / 2, to = 1/2, xlab="b", ylab=expression(mu(b)))
#use the mapping function "fred" to convert the sire effects from the
#canonical to mean-value parameter scale
bhat.sireGC.mu<- fred(bhat.sire)
GC_b2015<- bhat.sireGC.mu
stem(bhat.sireGC.mu)#no longer normal as expected
#generate probability density distribution
gc2015den<-density(bhat.sireGC.mu)
GC2015_den<- cbind(gc2015den[[1]], gc2015den[[2]])
#write.csv(GC2015_den, "gc2015den.csv", quote = FALSE, row.names = FALSE)
hoom<- predict(rout2015$obj, newcoef = rout2015$alpha, se.fit=TRUE)
goom <- hoom$gradient
moom<- goom[,5]
moom<- matrix(moom, ncol=5)
#this is additive genetic variation for fitness!
GC_Va<- 4*moom[1874 ,5]^2 * rout2015$nu[1]/map(0)
GC_Va #1.8576
#########################
#Estimate Va(W) for 2016#
#########################
#extract bhat - the estimates of random effects
bhat<- rout2016$b
#extract the sire effects
bhat.sire<- bhat[grep("paternalID", names(bhat))]
#stem plot of bhat estimates
stem(bhat.sire)#the canonical estimates "look" somewhat normal...as they should
hoom <- predict(rout2016$obj, newcoef = rout2016$alpha)
hoom<- matrix(hoom, ncol =5)
hoom<- hoom[ , 5]
#add a component of b_hat that we want to map to the mean value
#parameter scale (from canoonical scale) to fit:block1 fixed effect (alpha[11]= block6),
#predict, and take value of fourth node for first indiv.
map <- function(b) {
stopifnot(length(b) == 1)
stopifnot(is.finite(b))
alpha <- rout2016$alpha
alpha[11] <- alpha[11] + b #block 6 effects
hoom <- predict(rout2016$obj, newcoef = alpha)
hoom <- matrix(hoom, ncol = 5)
return(hoom[2274, 5])#first individual in block 6, fifth node
}
#vectorize mapping function
fred<- Vectorize(map)
#plot the curve
curve(fred, from = -1 / 2, to = 1/2, xlab="b", ylab=expression(mu(b)))
#use the mapping function "fred" to convert the sire effects from the
#canonical to mean-value parameter scale
bhat.sireGC.mu<- fred(bhat.sire)
GC_b2016<- bhat.sireGC.mu
stem(bhat.sireGC.mu)#no longer normal as expected
#generate probability density distribution
gc2016den<- density(bhat.sireGC.mu)
GC2016_den<- cbind(gc2016den[[1]], gc2016den[[2]])
#write.csv(GC2016_den, "gc2016den.csv", quote = FALSE, row.names = FALSE)
hoom<- predict(rout2016$obj, newcoef = rout2016$alpha, se.fit=TRUE)
goom <- hoom$gradient
moom<- goom[,5]
moom<- matrix(moom, ncol=5)
#this is additive genetic variation for fitness!
GC_Va<- 4*moom[2274 ,5]^2 * rout2016$nu[1]/map(0)
GC_Va #0.63142
#########################
#Estimate Va(W) for 2017#
#########################
#extract bhat - the estimates of random effects
bhat<- rout2017$b
#extract the sire effects
bhat.sire<- bhat[grep("paternalID", names(bhat))]
#stem plot of bhat estimates
stem(bhat.sire)#the canonical estimates "look" somewhat normal...as they should
hoom <- predict(rout2017$obj, newcoef = rout2017$alpha)
hoom<- matrix(hoom, ncol =5)
hoom<- hoom[ , 5]
#add a component of b_hat that we want to map to the mean value
#parameter scale (from canoonical scale) to fit:block1 fixed effect (alpha[12]= block7),
#predict, and take value of fourth node for first indiv.
map <- function(b) {
stopifnot(length(b) == 1)
stopifnot(is.finite(b))
alpha <- rout2017$alpha
alpha[12] <- alpha[12] + b #block 7 effects
hoom <- predict(rout2017$obj, newcoef = alpha)
hoom <- matrix(hoom, ncol = 5)
return(hoom[3126, 5])#first individual in block 7, fourth node
}
#vectorize mapping function
fred<- Vectorize(map)
#plot the curve
curve(fred, from = -1 / 2, to = 1/2, xlab="b", ylab=expression(mu(b)))
#use the mapping function "fred" to convert the sire effects from the
#canonical to mean-value parameter scale
bhat.sireGC.mu<- fred(bhat.sire)
GC_b2017<- bhat.sireGC.mu
GCb<- rbind(GC_b2015, GC_b2016, GC_b2017)
#write.table(GCb, "GCb_threeyears.csv", sep=",")
stem(bhat.sireGC.mu)#no longer normal as expected
#generate probability density distribution
gc2017den<- density(bhat.sireGC.mu)
GC2017_den<- cbind(gc2017den[[1]], gc2017den[[2]])
#write.csv(GC2017_den, "gc2017den.csv", quote = FALSE, row.names = FALSE)
hoom<- predict(rout2017$obj, newcoef = rout2017$alpha, se.fit=TRUE)
goom <- hoom$gradient
moom<- goom[,5]
moom<- matrix(moom, ncol=5)
#this is additive genetic variation for fitness!
GC_Va<- 4*moom[3126 ,5]^2 * rout2017$nu[1]/map(0)
GC_Va #6.49131
The following code calculates the additive genetic effects on total lifetime fitness
# from the three reaster output files generated in: GC_W_reaster_analyses.R
setwd("C:/Users/Mason Kulbaba/Dropbox/Rscripts/chamaecrista-adaptive-capacity/VaW_W_analyses/GC")
#Load reaster
#2015 reaster data
load(file="rout2015.RData")
#2016 reaster data
load(file="rout2016.RData")
#2017 reaster data
load(file="rout2017.RData")
library(aster)
#########################
#Estimate Va(W) for 2015#
#########################
#extract bhat - the estimates of random effects
bhat<- rout2015$b
#extract the sire effects
bhat.sire<- bhat[grep("paternalID", names(bhat))]
#stem plot of bhat estimates
stem(bhat.sire)#the canonical estimates "look" somewhat normal...as they should
hoom <- predict(rout2015$obj, newcoef = rout2015$alpha)
hoom<- matrix(hoom, ncol =5)
hoom<- hoom[ , 5]
#add a component of b_hat that we want to map to the mean value
#parameter scale (from canoonical scale) to fit:block1 fixed effect (alpha[11]= block6),
#predict, and take value of fourth node for first indiv.
map <- function(b) {
stopifnot(length(b) == 1)
stopifnot(is.finite(b))
alpha <- rout2015$alpha
alpha[11] <- alpha[11] + b #block 6 effects
hoom <- predict(rout2015$obj, newcoef = alpha)
hoom <- matrix(hoom, ncol = 5)
return(hoom[1874, 5])#first individual in block 6, fifth node
}
#vectorize mapping function
fred<- Vectorize(map)
#plot the curve
curve(fred, from = -1 / 2, to = 1/2, xlab="b", ylab=expression(mu(b)))
#use the mapping function "fred" to convert the sire effects from the
#canonical to mean-value parameter scale
bhat.sireGC.mu<- fred(bhat.sire)
GC_b2015<- bhat.sireGC.mu
stem(bhat.sireGC.mu)#no longer normal as expected
#generate probability density distribution
gc2015den<-density(bhat.sireGC.mu)
GC2015_den<- cbind(gc2015den[[1]], gc2015den[[2]])
#write.csv(GC2015_den, "gc2015den.csv", quote = FALSE, row.names = FALSE)
hoom<- predict(rout2015$obj, newcoef = rout2015$alpha, se.fit=TRUE)
goom <- hoom$gradient
moom<- goom[,5]
moom<- matrix(moom, ncol=5)
#this is additive genetic variation for fitness!
GC_Va<- 4*moom[1874 ,5]^2 * rout2015$nu[1]/map(0)
GC_Va #1.8576
#########################
#Estimate Va(W) for 2016#
#########################
#extract bhat - the estimates of random effects
bhat<- rout2016$b
#extract the sire effects
bhat.sire<- bhat[grep("paternalID", names(bhat))]
#stem plot of bhat estimates
stem(bhat.sire)#the canonical estimates "look" somewhat normal...as they should
hoom <- predict(rout2016$obj, newcoef = rout2016$alpha)
hoom<- matrix(hoom, ncol =5)
hoom<- hoom[ , 5]
#add a component of b_hat that we want to map to the mean value
#parameter scale (from canoonical scale) to fit:block1 fixed effect (alpha[11]= block6),
#predict, and take value of fourth node for first indiv.
map <- function(b) {
stopifnot(length(b) == 1)
stopifnot(is.finite(b))
alpha <- rout2016$alpha
alpha[11] <- alpha[11] + b #block 6 effects
hoom <- predict(rout2016$obj, newcoef = alpha)
hoom <- matrix(hoom, ncol = 5)
return(hoom[2274, 5])#first individual in block 6, fifth node
}
#vectorize mapping function
fred<- Vectorize(map)
#plot the curve
curve(fred, from = -1 / 2, to = 1/2, xlab="b", ylab=expression(mu(b)))
#use the mapping function "fred" to convert the sire effects from the
#canonical to mean-value parameter scale
bhat.sireGC.mu<- fred(bhat.sire)
GC_b2016<- bhat.sireGC.mu
stem(bhat.sireGC.mu)#no longer normal as expected
#generate probability density distribution
gc2016den<- density(bhat.sireGC.mu)
GC2016_den<- cbind(gc2016den[[1]], gc2016den[[2]])
#write.csv(GC2016_den, "gc2016den.csv", quote = FALSE, row.names = FALSE)
hoom<- predict(rout2016$obj, newcoef = rout2016$alpha, se.fit=TRUE)
goom <- hoom$gradient
moom<- goom[,5]
moom<- matrix(moom, ncol=5)
#this is additive genetic variation for fitness!
GC_Va<- 4*moom[2274 ,5]^2 * rout2016$nu[1]/map(0)
GC_Va #0.83021
###############
#extract bhat - the estimates of random effects
bhat<- rout2017$b
#extract the sire effects
bhat.sire<- bhat[grep("paternalID", names(bhat))]
#stem plot of bhat estimates
stem(bhat.sire)#the canonical estimates "look" somewhat normal...as they should
hoom <- predict(rout2017$obj, newcoef = rout2017$alpha)
hoom<- matrix(hoom, ncol =5)
hoom<- hoom[ , 5]
#add a component of b_hat that we want to map to the mean value
#parameter scale (from canoonical scale) to fit:block1 fixed effect (alpha[12]= block7),
#predict, and take value of fourth node for first indiv.
map <- function(b) {
stopifnot(length(b) == 1)
stopifnot(is.finite(b))
alpha <- rout2017$alpha
alpha[12] <- alpha[12] + b #block 7 effects
hoom <- predict(rout2017$obj, newcoef = alpha)
hoom <- matrix(hoom, ncol = 5)
return(hoom[3126, 5])#first individual in block 7, fourth node
}
#vectorize mapping function
fred<- Vectorize(map)
#plot the curve
curve(fred, from = -1 / 2, to = 1/2, xlab="b", ylab=expression(mu(b)))
#use the mapping function "fred" to convert the sire effects from the
#canonical to mean-value parameter scale
bhat.sireGC.mu<- fred(bhat.sire)
GC_b2017<- bhat.sireGC.mu
GCb<- rbind(GC_b2015, GC_b2016, GC_b2017)
#write.table(GCb, "GCb_threeyears.csv", sep=",")
stem(bhat.sireGC.mu)#no longer normal as expected
#generate probability density distribution
gc2017den<- density(bhat.sireGC.mu)
GC2017_den<- cbind(gc2017den[[1]], gc2017den[[2]])
#write.csv(GC2017_den, "gc2017den.csv", quote = FALSE, row.names = FALSE)
hoom<- predict(rout2017$obj, newcoef = rout2017$alpha, se.fit=TRUE)
goom <- hoom$gradient
moom<- goom[,5]
moom<- matrix(moom, ncol=5)
#this is additive genetic variation for fitness!
GC_Va<- 4*moom[3126 ,5]^2 * rout2017$nu[1]/map(0)
GC_Va #6.49131
